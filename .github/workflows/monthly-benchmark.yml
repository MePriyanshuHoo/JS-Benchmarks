name: Monthly Framework Benchmark

on:
  schedule:
    # Run on the 1st of every month at 02:00 UTC
    - cron: '0 2 1 * *'
  workflow_dispatch: # Allow manual triggering
    inputs:
      custom_config:
        description: 'Custom benchmark configuration (JSON)'
        required: false
        default: ''

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    strategy:
      matrix:
        node-version: [20, 22]
      fail-fast: false

    steps:
    - name: ğŸ›’ Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: ğŸ“‹ Setup Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'

    - name: ğŸŸ¡ Setup Bun
      uses: oven-sh/setup-bun@v1
      with:
        bun-version: latest

    - name: ğŸ“¦ Install Node.js dependencies
      run: |
        npm ci
        npm ls --depth=0

    - name: ğŸŸ¡ Install Bun dependencies
      run: |
        bun install --frozen-lockfile
        bun pm ls

    - name: ğŸ” Verify installations
      run: |
        echo "Node.js version: $(node --version)"
        echo "npm version: $(npm --version)"
        echo "Bun version: $(bun --version)"
        echo "Available frameworks:"
        ls -la *_server.js

    - name: ğŸ§ª Test server startup
      run: |
        echo "Testing server startup capabilities..."
        timeout 10s node express_server.js &
        sleep 3
        curl -f http://localhost:3000 || echo "Express test failed"
        pkill -f express_server.js || true
        sleep 1

        timeout 10s node fastify_server.js &
        sleep 3
        curl -f http://localhost:3001 || echo "Fastify test failed"
        pkill -f fastify_server.js || true
        sleep 1

        timeout 10s node hono_server.js &
        sleep 3
        curl -f http://localhost:3002 || echo "Hono test failed"
        pkill -f hono_server.js || true

    - name: ğŸƒâ€â™‚ï¸ Run comprehensive benchmark
      id: benchmark
      run: |
        echo "Starting comprehensive framework benchmark..."
        echo "Node.js version: ${{ matrix.node-version }}"

        # Create results directory for this Node version
        mkdir -p "results/node-${{ matrix.node-version }}"

        # Run the enhanced benchmark
        node scripts/benchmark.js 2>&1 | tee "results/node-${{ matrix.node-version }}/benchmark-output.log"

        # Check if benchmark completed successfully
        if [ -f "benchmark_results.json" ]; then
          echo "âœ… Benchmark completed successfully"
          cp benchmark_results.json "results/node-${{ matrix.node-version }}/benchmark_results.json"

          # Extract key metrics for summary
          echo "benchmark_success=true" >> $GITHUB_OUTPUT

          TOP_PERFORMER=$(jq -r '.rankings.byRequestsPerSecond[0].name' benchmark_results.json)
          TOP_RPS=$(jq -r '.rankings.byRequestsPerSecond[0].value' benchmark_results.json)

          echo "top_performer=${TOP_PERFORMER}" >> $GITHUB_OUTPUT
          echo "top_rps=${TOP_RPS}" >> $GITHUB_OUTPUT

        else
          echo "âŒ Benchmark failed - no results file generated"
          echo "benchmark_success=false" >> $GITHUB_OUTPUT
          exit 1
        fi

    - name: ğŸ“Š Generate performance summary
      if: steps.benchmark.outputs.benchmark_success == 'true'
      run: |
        echo "## ğŸ“Š Benchmark Results Summary (Node.js ${{ matrix.node-version }})" > benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo "**ğŸ† Top Performer:** ${{ steps.benchmark.outputs.top_performer }}" >> benchmark-summary.md
        echo "**ğŸ“ˆ Peak Performance:** $(printf "%.2f" ${{ steps.benchmark.outputs.top_rps }}) req/sec" >> benchmark-summary.md
        echo "" >> benchmark-summary.md
        echo "**ğŸ“… Test Date:** $(date -u)" >> benchmark-summary.md
        echo "**ğŸ–¥ï¸ Environment:** Ubuntu Latest, Node.js ${{ matrix.node-version }}" >> benchmark-summary.md
        echo "" >> benchmark-summary.md

        # Add top 3 performers
        echo "### ğŸ¥‡ Top 3 Performers" >> benchmark-summary.md
        jq -r '.rankings.byRequestsPerSecond[0:3][] | "- **\(.name)**: \(.value | tonumber | . * 100 | round / 100) req/sec"' benchmark_results.json >> benchmark-summary.md

    - name: ğŸ“ Generate updated README
      if: steps.benchmark.outputs.benchmark_success == 'true' && matrix.node-version == 22
      run: |
        echo "Generating updated README with latest benchmark results..."
        node scripts/generate-readme.js benchmark_results.json README.md

        # Verify README was generated
        if [ -f "README.md" ]; then
          echo "âœ… README generated successfully"
          echo "ğŸ“„ README size: $(wc -l < README.md) lines"
        else
          echo "âŒ README generation failed"
          exit 1
        fi

    - name: ğŸ—ƒï¸ Archive historical data
      if: steps.benchmark.outputs.benchmark_success == 'true' && matrix.node-version == 22
      run: |
        # Create historical data directory
        mkdir -p data/historical/$(date +%Y)

        # Archive results with timestamp
        TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)
        cp benchmark_results.json "data/historical/$(date +%Y)/benchmark_${TIMESTAMP}.json"

        # Create monthly summary
        echo "# Monthly Benchmark - $(date +%B_%Y)" > "data/historical/$(date +%Y)/summary_$(date +%Y-%m).md"
        cat benchmark-summary.md >> "data/historical/$(date +%Y)/summary_$(date +%Y-%m).md"

        echo "ğŸ“ Historical data archived to data/historical/"

    - name: ğŸ” Validate changes
      if: steps.benchmark.outputs.benchmark_success == 'true' && matrix.node-version == 22
      run: |
        echo "Validating generated files..."

        # Check README structure
        if grep -q "Performance Rankings" README.md && grep -q "Runtime Comparisons" README.md; then
          echo "âœ… README structure validated"
        else
          echo "âŒ README structure validation failed"
          exit 1
        fi

        # Check benchmark results
        if jq empty benchmark_results.json; then
          echo "âœ… Benchmark results JSON is valid"
        else
          echo "âŒ Benchmark results JSON is invalid"
          exit 1
        fi

    - name: ğŸ“¤ Commit and push changes
      if: steps.benchmark.outputs.benchmark_success == 'true' && matrix.node-version == 22
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Benchmark Bot"

        # Add changes
        git add README.md
        git add benchmark_results.json
        git add data/historical/ || true
        git add results/ || true

        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "ğŸ“‹ No changes to commit"
        else
          # Commit changes
          COMMIT_MSG="ğŸ¤– Monthly benchmark update - $(date +%B_%Y)

ğŸ“Š Benchmark Results:
- Top Performer: ${{ steps.benchmark.outputs.top_performer }}
- Peak Performance: $(printf "%.2f" ${{ steps.benchmark.outputs.top_rps }}) req/sec
- Node.js Version: ${{ matrix.node-version }}
- Test Date: $(date -u)

ğŸ”„ Auto-generated by GitHub Actions"

          git commit -m "$COMMIT_MSG"
          git push

          echo "âœ… Changes committed and pushed successfully"
        fi

    - name: ğŸ“‹ Create performance report
      if: steps.benchmark.outputs.benchmark_success == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-report-node${{ matrix.node-version }}
        path: |
          benchmark_results.json
          benchmark-summary.md
          results/
        retention-days: 90

    - name: ğŸ’¬ Comment on recent issues (if any)
      if: steps.benchmark.outputs.benchmark_success == 'true' && matrix.node-version == 22
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          // Read the benchmark summary
          const summary = fs.readFileSync('benchmark-summary.md', 'utf8');

          // Find recent issues with 'benchmark' label
          const issues = await github.rest.issues.listForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            labels: 'benchmark',
            state: 'open',
            sort: 'updated',
            per_page: 5
          });

          if (issues.data.length > 0) {
            const comment = `## ğŸ¤– Monthly Benchmark Update

The automated benchmark has completed! Here are the latest results:

${summary}

View the complete results in the [updated README](https://github.com/${context.repo.owner}/${context.repo.repo}#readme).

_This comment was automatically generated by the monthly benchmark workflow._`;

            // Comment on the most recent benchmark-related issue
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: issues.data[0].number,
              body: comment
            });
          }

  # Summary job that runs after all matrix jobs complete
  summary:
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()

    steps:
    - name: ğŸ“Š Benchmark Summary
      run: |
        echo "## ğŸ¯ Monthly Benchmark Workflow Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**ğŸ“… Execution Date:** $(date -u)" >> $GITHUB_STEP_SUMMARY
        echo "**ğŸ”„ Workflow:** Monthly Framework Benchmark" >> $GITHUB_STEP_SUMMARY
        echo "**ğŸ“‹ Status:** ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ needs.benchmark.result }}" == "success" ]; then
          echo "âœ… **Result:** Benchmark completed successfully across all Node.js versions" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ“ˆ **Next Steps:**" >> $GITHUB_STEP_SUMMARY
          echo "- README.md has been updated with latest results" >> $GITHUB_STEP_SUMMARY
          echo "- Historical data archived for trend analysis" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark artifacts available for download" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Result:** Benchmark workflow encountered issues" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ”§ **Recommended Actions:**" >> $GITHUB_STEP_SUMMARY
          echo "- Check workflow logs for detailed error information" >> $GITHUB_STEP_SUMMARY
          echo "- Verify framework server implementations" >> $GITHUB_STEP_SUMMARY
          echo "- Consider running workflow manually for debugging" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“Š **View Results:** [Updated README](https://github.com/${{ github.repository }}#readme)" >> $GITHUB_STEP_SUMMARY

    - name: ğŸš¨ Notify on failure
      if: needs.benchmark.result == 'failure'
      uses: actions/github-script@v7
      with:
        script: |
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ğŸš¨ Monthly Benchmark Workflow Failed',
            body: `## ğŸš¨ Monthly Benchmark Failure Report

The automated monthly benchmark workflow has failed.

**ğŸ“… Failure Date:** ${new Date().toISOString()}
**ğŸ”— Workflow Run:** https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}

### ğŸ” Recommended Actions

1. **Check the workflow logs** for detailed error information
2. **Verify server implementations** are working correctly
3. **Test framework installations** and dependencies
4. **Run the benchmark manually** to reproduce the issue

### ğŸ› ï¸ Quick Debug Commands

\`\`\`bash
# Test individual servers
npm run express:node
npm run fastify:node
npm run hono:node

# Run benchmark manually
node scripts/benchmark.js
\`\`\`

This issue was automatically created by the benchmark workflow. Please investigate and resolve the underlying cause.

/label bug,benchmark,automation`,
            labels: ['bug', 'benchmark', 'automation']
          });

          console.log(`Created issue #${issue.data.number} for benchmark failure`);
