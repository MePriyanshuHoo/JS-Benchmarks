name: Pull Request Benchmark

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - '**_server.js'
      - 'scripts/**'
      - 'package*.json'
      - 'bun.lock'
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to benchmark'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: write

jobs:
  pr-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: ðŸ›’ Checkout PR code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: ðŸ“‹ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '22'
        cache: 'npm'

    - name: ðŸŸ¡ Setup Bun
      uses: oven-sh/setup-bun@v1
      with:
        bun-version: latest

    - name: ðŸ“¦ Install dependencies
      run: |
        npm ci
        bun install --frozen-lockfile

    - name: ðŸ” Check what changed
      id: changes
      run: |
        echo "Analyzing changes in this PR..."

        CHANGED_FILES=$(git diff --name-only HEAD~1)
        echo "Changed files:"
        echo "$CHANGED_FILES"

        # Check if server files changed
        if echo "$CHANGED_FILES" | grep -E ".*_server\.js$"; then
          echo "server_changed=true" >> $GITHUB_OUTPUT
          echo "ðŸ”„ Server files changed - full benchmark needed"
        else
          echo "server_changed=false" >> $GITHUB_OUTPUT
          echo "ðŸ“‹ No server changes - light benchmark"
        fi

        # Check if benchmark scripts changed
        if echo "$CHANGED_FILES" | grep -E "scripts/.*\.js$"; then
          echo "benchmark_changed=true" >> $GITHUB_OUTPUT
          echo "ðŸ”§ Benchmark scripts changed"
        else
          echo "benchmark_changed=false" >> $GITHUB_OUTPUT
        fi

    - name: ðŸ§ª Quick server validation
      run: |
        echo "Testing server startup..."

        # Test each server can start
        timeout 10s node express_server.js &
        sleep 2 && curl -f http://localhost:3000 > /dev/null && echo "âœ… Express OK"
        pkill -f express_server.js || true
        sleep 1

        timeout 10s node fastify_server.js &
        sleep 2 && curl -f http://localhost:3001 > /dev/null && echo "âœ… Fastify OK"
        pkill -f fastify_server.js || true
        sleep 1

        timeout 10s node hono_server.js &
        sleep 2 && curl -f http://localhost:3002 > /dev/null && echo "âœ… Hono OK"
        pkill -f hono_server.js || true

    - name: ðŸƒâ€â™‚ï¸ Run PR benchmark (light)
      if: steps.changes.outputs.server_changed == 'false'
      run: |
        echo "Running light benchmark for PR validation..."

        # Create a light benchmark config
        cat > light-benchmark.js << 'EOF'
        const { BenchmarkRunner } = require('./scripts/benchmark.js');

        class LightBenchmarkRunner extends BenchmarkRunner {
          constructor() {
            super();
            // Light configuration for PR testing
            this.config = {
              ...this.config,
              connections: 50,    // Reduced connections
              duration: 10,       // Shorter duration
              runs: 1,           // Single run
              warmupTime: 2000,  // Shorter warmup
              cooldownTime: 1000 // Shorter cooldown
            };
          }
        }

        async function main() {
          const benchmark = new LightBenchmarkRunner();
          const report = await benchmark.runAllBenchmarks();
          benchmark.printSummary();
          await benchmark.saveResults('pr_benchmark_results.json');
        }

        main().catch(console.error);
        EOF

        node light-benchmark.js

    - name: ðŸƒâ€â™‚ï¸ Run PR benchmark (full)
      if: steps.changes.outputs.server_changed == 'true'
      run: |
        echo "Running full benchmark due to server changes..."

        # Use enhanced benchmark but with reduced config for CI
        cat > pr-benchmark.js << 'EOF'
        const { BenchmarkRunner } = require('./scripts/benchmark.js');

        class PRBenchmarkRunner extends BenchmarkRunner {
          constructor() {
            super();
            // CI-optimized configuration
            this.config = {
              ...this.config,
              connections: 75,    // Moderate connections
              duration: 20,       // Moderate duration
              runs: 2,           // Two runs for better accuracy
              warmupTime: 2000,
              cooldownTime: 1500
            };
          }
        }

        async function main() {
          const benchmark = new PRBenchmarkRunner();
          const report = await benchmark.runAllBenchmarks();
          benchmark.printSummary();
          await benchmark.saveResults('pr_benchmark_results.json');
        }

        main().catch(console.error);
        EOF

        node pr-benchmark.js

    - name: ðŸ“Š Generate PR benchmark report
      if: always()
      run: |
        if [ ! -f "pr_benchmark_results.json" ]; then
          echo "âŒ No benchmark results found"
          exit 1
        fi

        # Create a concise PR report
        cat > pr-report.md << 'EOF'
## ðŸš€ PR Benchmark Results

EOF

        # Add benchmark type info
        if [ "${{ steps.changes.outputs.server_changed }}" == "true" ]; then
          echo "**ðŸ”„ Benchmark Type:** Full (server changes detected)" >> pr-report.md
        else
          echo "**ðŸ“‹ Benchmark Type:** Light (no server changes)" >> pr-report.md
        fi

        echo "" >> pr-report.md
        echo "**ðŸ“… Test Date:** $(date -u)" >> pr-report.md
        echo "**ðŸ–¥ï¸ Environment:** Ubuntu Latest, Node.js 22" >> pr-report.md
        echo "" >> pr-report.md

        # Extract top performers
        echo "### ðŸ† Top Performers" >> pr-report.md
        jq -r '.rankings.byRequestsPerSecond[0:3][] | "- **\(.name)**: \(.value | tonumber | . * 100 | round / 100) req/sec"' pr_benchmark_results.json >> pr-report.md

        echo "" >> pr-report.md
        echo "### âš¡ Runtime Comparisons" >> pr-report.md

        # Runtime improvements
        jq -r 'to_entries[] | select(.key != "metadata" and .key != "environment" and .key != "configuration" and .key != "summary" and .key != "results" and .key != "rankings") | "\(.key): \(.value.improvement.percentage | tonumber | . * 100 | round / 100)% improvement with Bun"' pr_benchmark_results.json > /tmp/improvements.txt 2>/dev/null || echo "No runtime comparisons available" > /tmp/improvements.txt

        if [ -s /tmp/improvements.txt ]; then
          cat /tmp/improvements.txt >> pr-report.md
        fi

        echo "" >> pr-report.md
        echo "### ðŸ“‹ Configuration" >> pr-report.md
        echo "- **Connections:** $(jq -r '.configuration.connections' pr_benchmark_results.json)" >> pr-report.md
        echo "- **Duration:** $(jq -r '.configuration.duration' pr_benchmark_results.json) seconds" >> pr-report.md
        echo "- **Runs:** $(jq -r '.configuration.runs' pr_benchmark_results.json)" >> pr-report.md

        echo "" >> pr-report.md
        echo "_This benchmark was automatically run to validate your changes. For the complete benchmark suite, check the monthly automated reports._" >> pr-report.md

    - name: ðŸ” Compare with baseline (if available)
      continue-on-error: true
      run: |
        echo "Looking for baseline benchmark data..."

        # Try to fetch the latest benchmark from main branch
        git fetch origin main:main 2>/dev/null || true

        if git show main:benchmark_results.json > baseline_results.json 2>/dev/null; then
          echo "âœ… Found baseline benchmark data"

          # Create comparison report
          cat > comparison.js << 'EOF'
          const fs = require('fs');

          try {
            const baseline = JSON.parse(fs.readFileSync('baseline_results.json', 'utf8'));
            const current = JSON.parse(fs.readFileSync('pr_benchmark_results.json', 'utf8'));

            console.log('\n## ðŸ“ˆ Performance Comparison vs Main Branch\n');

            // Compare top performers
            const baselineTop = baseline.rankings?.byRequestsPerSecond?.[0];
            const currentTop = current.rankings?.byRequestsPerSecond?.[0];

            if (baselineTop && currentTop) {
              const improvement = ((currentTop.value - baselineTop.value) / baselineTop.value) * 100;
              const emoji = improvement > 0 ? 'ðŸ“ˆ' : improvement < 0 ? 'ðŸ“‰' : 'âž¡ï¸';

              console.log(`**Top Performer Change:**`);
              console.log(`- Baseline: ${baselineTop.name} (${baselineTop.value.toFixed(2)} req/sec)`);
              console.log(`- Current: ${currentTop.name} (${currentTop.value.toFixed(2)} req/sec)`);
              console.log(`- Change: ${emoji} ${improvement.toFixed(2)}%`);
            }

          } catch (error) {
            console.log('âš ï¸ Could not generate detailed comparison');
          }
          EOF

          node comparison.js >> pr-report.md
        else
          echo "âš ï¸ No baseline data available for comparison"
          echo "" >> pr-report.md
          echo "âš ï¸ _No baseline data available for performance comparison_" >> pr-report.md
        fi

    - name: ðŸ’¬ Post PR comment
      uses: actions/github-script@v7
      if: always() && github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');

          try {
            const report = fs.readFileSync('pr-report.md', 'utf8');

            // Check if we already commented on this PR
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.data.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸš€ PR Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report
              });
              console.log('Updated existing benchmark comment');
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
              console.log('Created new benchmark comment');
            }
          } catch (error) {
            console.error('Failed to post comment:', error);

            // Fallback comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `## ðŸš€ PR Benchmark Results

âŒ Benchmark completed but failed to generate detailed report.

Please check the [workflow logs](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`
            });
          }

    - name: ðŸ“¤ Upload PR benchmark artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pr-benchmark-results
        path: |
          pr_benchmark_results.json
          pr-report.md
          baseline_results.json
        retention-days: 30

    - name: âœ… Summary
      run: |
        echo "## ðŸŽ¯ PR Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ -f "pr_benchmark_results.json" ]; then
          echo "âœ… **Status:** Benchmark completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          TOP_PERFORMER=$(jq -r '.rankings.byRequestsPerSecond[0].name' pr_benchmark_results.json)
          TOP_RPS=$(jq -r '.rankings.byRequestsPerSecond[0].value' pr_benchmark_results.json)

          echo "ðŸ† **Top Performer:** $TOP_PERFORMER" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ˆ **Peak Performance:** $(printf "%.2f" $TOP_RPS) req/sec" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ’¬ **Next Steps:** Check the PR comment for detailed results" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **Status:** Benchmark failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”§ **Action Required:** Check workflow logs and fix issues" >> $GITHUB_STEP_SUMMARY
        fi
