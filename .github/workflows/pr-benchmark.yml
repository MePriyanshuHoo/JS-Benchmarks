name: Pull Request Benchmark (C++)

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - '**_server.js'
      - 'benchmark_wrk.cpp'
      - 'Makefile'
      - 'package*.json'
      - 'bun.lock'
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to benchmark'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: write

jobs:
  pr-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - name: 🛒 Checkout PR code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: 📋 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '22'
        cache: 'npm'

    - name: 🟡 Setup Bun
      uses: oven-sh/setup-bun@v1
      with:
        bun-version: latest

    - name: 🔧 Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libcurl4-openssl-dev pkg-config
        echo "✅ Build tools installed"

    - name: ⚡ Install WRK
      run: |
        echo "Installing WRK for PR benchmarking..."
        git clone https://github.com/wg/wrk.git /tmp/wrk
        cd /tmp/wrk
        make
        sudo cp wrk /usr/local/bin/
        cd $GITHUB_WORKSPACE
        rm -rf /tmp/wrk
        wrk --version || echo "WRK installed"
        echo "✅ WRK installed"

    - name: 📦 Install framework dependencies
      run: |
        npm install
        bun install --frozen-lockfile
        echo "✅ Framework dependencies installed"

    - name: 🔍 Analyze changes
      id: changes
      run: |
        echo "Analyzing PR changes..."

        CHANGED_FILES=$(git diff --name-only HEAD~1)
        echo "Changed files:"
        echo "$CHANGED_FILES"

        # Check if server files changed
        if echo "$CHANGED_FILES" | grep -E ".*_server\.js$"; then
          echo "server_changed=true" >> $GITHUB_OUTPUT
          echo "🔄 Server files changed - full benchmark needed"
          echo "benchmark_type=full" >> $GITHUB_OUTPUT
        else
          echo "server_changed=false" >> $GITHUB_OUTPUT
          echo "📋 No server changes - light benchmark"
          echo "benchmark_type=light" >> $GITHUB_OUTPUT
        fi

        # Check if C++ benchmark changed
        if echo "$CHANGED_FILES" | grep -E "(benchmark_wrk\.cpp|Makefile)"; then
          echo "cpp_changed=true" >> $GITHUB_OUTPUT
          echo "🔧 C++ benchmark files changed"
        else
          echo "cpp_changed=false" >> $GITHUB_OUTPUT
        fi

    - name: 🔨 Build C++ benchmark (PR optimized)
      run: |
        echo "Building C++ benchmark for PR validation..."

        # Create PR-optimized version
        cp benchmark_wrk.cpp benchmark_wrk_pr.cpp

        # Modify config for faster PR testing (cross-platform sed)
        if [[ "$OSTYPE" == "darwin"* ]]; then
          # macOS
          sed -i '' 's/connections = 100/connections = 50/' benchmark_wrk_pr.cpp
          sed -i '' 's/duration = "30s"/duration = "10s"/' benchmark_wrk_pr.cpp
          sed -i '' 's/runs = 3/runs = 1/' benchmark_wrk_pr.cpp
          sed -i '' 's/warmupTime = 3000/warmupTime = 2000/' benchmark_wrk_pr.cpp
          sed -i '' 's/cooldownTime = 2000/cooldownTime = 1000/' benchmark_wrk_pr.cpp
        else
          # Linux
          sed -i 's/connections = 100/connections = 50/' benchmark_wrk_pr.cpp
          sed -i 's/duration = "30s"/duration = "10s"/' benchmark_wrk_pr.cpp
          sed -i 's/runs = 3/runs = 1/' benchmark_wrk_pr.cpp
          sed -i 's/warmupTime = 3000/warmupTime = 2000/' benchmark_wrk_pr.cpp
          sed -i 's/cooldownTime = 2000/cooldownTime = 1000/' benchmark_wrk_pr.cpp
        fi

        # For light benchmark, make it even faster
        if [ "${{ steps.changes.outputs.benchmark_type }}" == "light" ]; then
          if [[ "$OSTYPE" == "darwin"* ]]; then
            # macOS
            sed -i '' 's/connections = 50/connections = 25/' benchmark_wrk_pr.cpp
            sed -i '' 's/duration = "10s"/duration = "5s"/' benchmark_wrk_pr.cpp
          else
            # Linux
            sed -i 's/connections = 50/connections = 25/' benchmark_wrk_pr.cpp
            sed -i 's/duration = "10s"/duration = "5s"/' benchmark_wrk_pr.cpp
          fi
        fi

        # Compile PR version
        g++ -std=c++17 -Wall -Wextra -O2 -pthread benchmark_wrk_pr.cpp -lcurl -o benchmark_wrk_pr

        echo "✅ PR-optimized C++ benchmark compiled"
        ls -la benchmark_wrk_pr
        file benchmark_wrk_pr

    - name: 🧪 Quick server validation
      run: |
        echo "Testing server startup capabilities..."

        # Test Express
        timeout 8s node express_server.js &
        EXPRESS_PID=$!
        sleep 2
        if curl -f http://localhost:3000 >/dev/null 2>&1; then
          echo "✅ Express server OK"
        else
          echo "❌ Express server failed"
        fi
        kill $EXPRESS_PID 2>/dev/null || true
        sleep 1

        # Test Fastify
        timeout 8s node fastify_server.js &
        FASTIFY_PID=$!
        sleep 2
        if curl -f http://localhost:3001 >/dev/null 2>&1; then
          echo "✅ Fastify server OK"
        else
          echo "❌ Fastify server failed"
        fi
        kill $FASTIFY_PID 2>/dev/null || true
        sleep 1

        # Test Hono
        timeout 8s node hono_server.js &
        HONO_PID=$!
        sleep 2
        if curl -f http://localhost:3002 >/dev/null 2>&1; then
          echo "✅ Hono server OK"
        else
          echo "❌ Hono server failed"
        fi
        kill $HONO_PID 2>/dev/null || true

        echo "✅ Server validation completed"

    - name: 🏃‍♂️ Run C++ PR benchmark
      id: benchmark
      run: |
        echo "Running C++ WRK benchmark for PR validation..."
        echo "Benchmark type: ${{ steps.changes.outputs.benchmark_type }}"

        # Set environment
        export NODE_ENV=production

        # Run the PR-optimized benchmark
        echo "Executing PR-optimized C++ benchmark..."
        ./benchmark_wrk_pr 2>&1 | tee pr_benchmark_output.log

        # Check results
        if [ -f "benchmark_results_wrk.json" ]; then
          echo "✅ C++ PR benchmark completed successfully"
          mv benchmark_results_wrk.json pr_benchmark_results.json
          mv benchmark_results_wrk.csv pr_benchmark_results.csv

          echo "benchmark_success=true" >> $GITHUB_OUTPUT

          # Extract key metrics
          TOP_PERFORMER=$(jq -r '.results | sort_by(.requestsPerSecond) | reverse | .[0].environment' pr_benchmark_results.json)
          TOP_RPS=$(jq -r '.results | sort_by(.requestsPerSecond) | reverse | .[0].requestsPerSecond' pr_benchmark_results.json)

          echo "top_performer=${TOP_PERFORMER}" >> $GITHUB_OUTPUT
          echo "top_rps=${TOP_RPS}" >> $GITHUB_OUTPUT

          echo "📊 PR Benchmark Results:"
          echo "  Top performer: $TOP_PERFORMER"
          echo "  Peak RPS: $TOP_RPS"

        else
          echo "❌ C++ PR benchmark failed - no results generated"
          echo "benchmark_success=false" >> $GITHUB_OUTPUT
          exit 1
        fi

    - name: 📊 Generate PR benchmark report
      if: steps.benchmark.outputs.benchmark_success == 'true'
      run: |
        echo "Generating PR benchmark report..."

        cat > pr-report.md << 'EOF'
## 🚀 C++ WRK PR Benchmark Results

EOF

        # Add benchmark type and configuration
        echo "**⚡ Benchmark Tool:** WRK (C++ Implementation)" >> pr-report.md
        if [ "${{ steps.changes.outputs.benchmark_type }}" == "full" ]; then
          echo "**🔄 Benchmark Type:** Full (server changes detected)" >> pr-report.md
          echo "**⚙️ Configuration:** 50 connections, 10s duration, 1 run" >> pr-report.md
        else
          echo "**📋 Benchmark Type:** Light (no server changes)" >> pr-report.md
          echo "**⚙️ Configuration:** 25 connections, 5s duration, 1 run" >> pr-report.md
        fi

        echo "" >> pr-report.md
        echo "**📅 Test Date:** $(date -u)" >> pr-report.md
        echo "**🖥️ Environment:** Ubuntu Latest, Node.js 22" >> pr-report.md
        echo "**🔧 Compiler:** $(gcc --version | head -1)" >> pr-report.md
        echo "" >> pr-report.md

        # Top performer
        echo "### 🏆 Top Performer" >> pr-report.md
        echo "**${{ steps.benchmark.outputs.top_performer }}**: $(printf "%.2f" ${{ steps.benchmark.outputs.top_rps }}) req/sec" >> pr-report.md
        echo "" >> pr-report.md

        # Performance table
        echo "### 📈 Performance Results" >> pr-report.md
        echo "| Framework | Runtime | RPS | Avg Latency | P90 Latency | P99 Latency | Throughput |" >> pr-report.md
        echo "|-----------|---------|-----|-------------|-------------|-------------|------------|" >> pr-report.md
        jq -r '.results | sort_by(.requestsPerSecond) | reverse | .[] | "| \(.framework) | \(.runtime) | \(.requestsPerSecond | . * 100 | round / 100) | \(.avgLatency | . * 100 | round / 100)ms | \(.p90Latency | . * 100 | round / 100)ms | \(.p99Latency | . * 100 | round / 100)ms | \(.throughput / 1024 / 1024 | . * 100 | round / 100)MB/s |"' pr_benchmark_results.json >> pr-report.md

        echo "" >> pr-report.md
        echo "### 🔄 Runtime Comparisons" >> pr-report.md

        # Node.js vs Bun comparisons
        FRAMEWORKS=$(jq -r '.results | map(.framework) | unique | .[]' pr_benchmark_results.json)
        for framework in $FRAMEWORKS; do
          NODE_RPS=$(jq -r --arg fw "$framework" '.results | map(select(.framework == $fw and .runtime == "node")) | .[0].requestsPerSecond // 0' pr_benchmark_results.json)
          BUN_RPS=$(jq -r --arg fw "$framework" '.results | map(select(.framework == $fw and .runtime == "bun")) | .[0].requestsPerSecond // 0' pr_benchmark_results.json)

          if [ "$NODE_RPS" != "0" ] && [ "$BUN_RPS" != "0" ]; then
            IMPROVEMENT=$(echo "scale=1; (($BUN_RPS - $NODE_RPS) / $NODE_RPS) * 100" | bc -l)
            echo "- **$framework**: Bun shows ${IMPROVEMENT}% performance vs Node.js" >> pr-report.md
          fi
        done

        echo "" >> pr-report.md
        echo "### 🔍 PR Validation" >> pr-report.md

        if [ "${{ steps.changes.outputs.server_changed }}" == "true" ]; then
          echo "✅ **Server changes detected** - Full benchmark executed to validate performance impact" >> pr-report.md
        else
          echo "✅ **No server changes** - Light benchmark confirms no performance regression" >> pr-report.md
        fi

        if [ "${{ steps.changes.outputs.cpp_changed }}" == "true" ]; then
          echo "🔧 **C++ benchmark changes detected** - Benchmark infrastructure updated" >> pr-report.md
        fi

        echo "" >> pr-report.md
        echo "---" >> pr-report.md
        echo "_This benchmark was automatically run using a high-performance C++ implementation with WRK. Results are optimized for PR validation and may differ from monthly comprehensive benchmarks._" >> pr-report.md

    - name: 🔍 Compare with baseline (if available)
      continue-on-error: true
      if: steps.benchmark.outputs.benchmark_success == 'true'
      run: |
        echo "Looking for baseline benchmark data..."

        # Try to fetch baseline from main branch
        git fetch origin main:main 2>/dev/null || true

        if git show main:benchmark_results_wrk.json > baseline_results.json 2>/dev/null; then
          echo "✅ Found baseline C++ WRK benchmark data"

          echo "" >> pr-report.md
          echo "## 📈 Performance Comparison vs Main Branch" >> pr-report.md
          echo "" >> pr-report.md

          # Compare top performers
          BASELINE_TOP=$(jq -r '.results | sort_by(.requestsPerSecond) | reverse | .[0].environment' baseline_results.json)
          BASELINE_RPS=$(jq -r '.results | sort_by(.requestsPerSecond) | reverse | .[0].requestsPerSecond' baseline_results.json)
          CURRENT_TOP="${{ steps.benchmark.outputs.top_performer }}"
          CURRENT_RPS="${{ steps.benchmark.outputs.top_rps }}"

          echo "**Baseline (main):** $BASELINE_TOP ($(printf "%.2f" $BASELINE_RPS) req/sec)" >> pr-report.md
          echo "**Current (PR):** $CURRENT_TOP ($(printf "%.2f" $CURRENT_RPS) req/sec)" >> pr-report.md

          IMPROVEMENT=$(echo "scale=2; (($CURRENT_RPS - $BASELINE_RPS) / $BASELINE_RPS) * 100" | bc -l)

          if (( $(echo "$IMPROVEMENT > 5" | bc -l) )); then
            echo "**Result:** 📈 **Significant improvement** (+${IMPROVEMENT}%)" >> pr-report.md
          elif (( $(echo "$IMPROVEMENT < -5" | bc -l) )); then
            echo "**Result:** 📉 **Performance regression** (${IMPROVEMENT}%)" >> pr-report.md
          else
            echo "**Result:** ➡️ **No significant change** (${IMPROVEMENT}%)" >> pr-report.md
          fi

          echo "" >> pr-report.md
          echo "⚠️ _Note: PR benchmarks use reduced load for faster CI execution. For precise comparisons, refer to monthly comprehensive benchmarks._" >> pr-report.md

        else
          echo "⚠️ No baseline C++ benchmark data available"
          echo "" >> pr-report.md
          echo "⚠️ _No baseline data available for performance comparison. This appears to be the first C++ benchmark._" >> pr-report.md
        fi

    - name: 💬 Post PR comment
      uses: actions/github-script@v7
      if: always() && github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');

          try {
            let report;
            if (fs.existsSync('pr-report.md')) {
              report = fs.readFileSync('pr-report.md', 'utf8');
            } else {
              // Fallback report
              report = `## 🚀 C++ WRK PR Benchmark Results

❌ Benchmark failed to complete successfully.

**📅 Test Date:** ${new Date().toISOString()}
**🔧 Implementation:** C++ with WRK load testing

Please check the [workflow logs](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.

### 🛠️ Common Issues
- C++ compilation errors
- Missing system dependencies (libcurl, build-essential)
- WRK installation problems
- Server startup failures

### 🔧 Local Testing
\`\`\`bash
# Install dependencies
make install-deps

# Build C++ benchmark
make clean && make

# Run benchmark
make run
\`\`\``;
            }

            // Check for existing benchmark comments
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.data.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('🚀 C++ WRK PR Benchmark Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report
              });
              console.log('✅ Updated existing PR benchmark comment');
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
              console.log('✅ Created new PR benchmark comment');
            }
          } catch (error) {
            console.error('❌ Failed to post comment:', error);

            // Emergency fallback comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: `## 🚀 C++ WRK PR Benchmark

❌ Benchmark completed but failed to generate report.

Check [workflow logs](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`
            });
          }

    - name: 📤 Upload PR benchmark artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: cpp-pr-benchmark-results
        path: |
          pr_benchmark_results.json
          pr_benchmark_results.csv
          pr-report.md
          pr_benchmark_output.log
          baseline_results.json
          benchmark_wrk_pr
        retention-days: 14

    - name: ✅ Summary
      run: |
        echo "## 🎯 C++ PR Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ steps.benchmark.outputs.benchmark_success }}" == "true" ]; then
          echo "✅ **Status:** C++ WRK benchmark completed successfully" >> $GITHUB_STEP_SUMMARY
          echo "⚡ **Implementation:** High-performance C++ with WRK load testing" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "🏆 **Top Performer:** ${{ steps.benchmark.outputs.top_performer }}" >> $GITHUB_STEP_SUMMARY
          echo "📈 **Peak Performance:** $(printf "%.2f" ${{ steps.benchmark.outputs.top_rps }}) req/sec" >> $GITHUB_STEP_SUMMARY
          echo "🔧 **Benchmark Type:** ${{ steps.changes.outputs.benchmark_type }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "💬 **Next Steps:** Check the PR comment for detailed performance analysis" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Status:** C++ benchmark failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "🔧 **Possible Issues:**" >> $GITHUB_STEP_SUMMARY
          echo "- C++ compilation errors" >> $GITHUB_STEP_SUMMARY
          echo "- Missing system dependencies" >> $GITHUB_STEP_SUMMARY
          echo "- WRK installation problems" >> $GITHUB_STEP_SUMMARY
          echo "- Framework server failures" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📋 **Action Required:** Review workflow logs and fix underlying issues" >> $GITHUB_STEP_SUMMARY
        fi
